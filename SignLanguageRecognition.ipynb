{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33510535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilobabic/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c104da",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/Users/danilobabic/Documents/GitHub/ri_train_data'\n",
    "words = []\n",
    "missing_videos = []\n",
    "video_metadata = []\n",
    "words_file = '/Users/danilobabic/Documents/GitHub/archive/wlasl_class_list.txt'\n",
    "missing_videos_file = '/Users/danilobabic/Documents/GitHub/archive/missing.txt'\n",
    "video_metadata_file = '/Users/danilobabic/Documents/GitHub/archive/WLASL_v0.3.json'\n",
    "archive_path = '/Users/danilobabic/Documents/GitHub/archive/videos'\n",
    "testing_path = '/Users/danilobabic/Documents/GitHub/testing'\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ace583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip().split('\\t')\n",
    "            words.append(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb8cecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_words(words_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63ae0909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_files(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            missing_videos.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af4d8969",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_missing_files(missing_videos_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12b6ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_metadata(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea98ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadata = get_video_metadata(video_metadata_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b7178f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_folders(path):\n",
    "    for word in words:\n",
    "        folder_path = os.path.join(path, word)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1814f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(frame, model):         \n",
    "    results = model.process(frame)                 \n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fad8c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_videos():\n",
    "    #print(\"USO U FUNKCIJUUUUU\")\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        print(video_metadata)\n",
    "        # for each word we find videos \n",
    "        for word in video_metadata:\n",
    "            for video in word['instances']:\n",
    "                if video['video_id'] in missing_videos or video['split'] != 'train': \n",
    "                    continue\n",
    "                video_path = os.path.join(folder_path, word['gloss'], video['video_id'])\n",
    "                os.makedirs(video_path, exist_ok=True)\n",
    "                cap = cv2.VideoCapture(archive_path + \"/\" + video['video_id'] + \".mp4\")\n",
    "                print(\"USO U FUNCKIJU FAJL :\" + archive_path + \"/\" + video['video_id'] + \".mp4\")\n",
    "                if video['frame_end'] == -1:\n",
    "                    frame_num = 1\n",
    "                    while True: \n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            break\n",
    "                        keypoints = extract_keypoints(frame, holistic)\n",
    "                        npy_path = os.path.join(video_path, str(frame_num))\n",
    "                        np.save(npy_path, keypoints)\n",
    "                        frame_num += 1\n",
    "                else:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, video['frame_start'])\n",
    "                    frame_num = video['frame_start']\n",
    "                    while frame_num <= video['frame_end']:\n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            break \n",
    "                        keypoints = extract_keypoints(frame, holistic)\n",
    "                        npy_path = os.path.join(video_path, str(frame_num))\n",
    "                        np.save(npy_path, keypoints)\n",
    "                        frame_num += 1 \n",
    "                        if frame_num > video['frame_end']:\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da308827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_folders():\n",
    "    frames = []\n",
    "    labels = []\n",
    "    word_folders = sorted([f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f)) and f != '.DS_Store'])\n",
    "    #testing_list = [\"book\",\"minute\",\"cherry\"]\n",
    "    for word_folder in word_folders:\n",
    "        word_path = os.path.join(folder_path, word_folder)\n",
    "        video_folders = sorted([f for f in os.listdir(word_path) if os.path.isdir(os.path.join(word_path, f)) and f != '.DS_Store'])\n",
    "        for video_folder in video_folders:\n",
    "            video_path = os.path.join(word_path, video_folder)\n",
    "            frame_files = sorted(os.listdir(video_path))\n",
    "            video_frames = []\n",
    "            for frame_file in frame_files:\n",
    "                frame_path = os.path.join(video_path, frame_file)\n",
    "                frame = np.load(frame_path)\n",
    "                video_frames.append(frame)\n",
    "            frames.append(video_frames)\n",
    "            labels.append(label_map[word_folder])\n",
    "    return frames, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be996211",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c90a1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8313, 233, 1662)\n",
      "(8313, 2000)\n"
     ]
    }
   ],
   "source": [
    "frames, labels = load_data_from_folders()\n",
    "#print(frames)\n",
    "max_frame_length = max(len(frame) for frame in frames)\n",
    "#print(labels)\n",
    "# Pad the frames to have a common length\n",
    "#padded_frames = tf.keras.preprocessing.sequence.pad_sequences(frames, maxlen=max_frame_length, padding='post', value=0.0)\n",
    "frames = tf.keras.preprocessing.sequence.pad_sequences(frames, padding='post', dtype='float32')\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "# Convert the padded_frames to a NumPy array\n",
    "frames_array = np.array(frames)\n",
    "labels_array = np.array(labels)\n",
    "print(frames_array.shape)\n",
    "print(labels_array.shape)\n",
    "#print(labels_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f286d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(64, return_sequences=True, activation='relu', input_shape=(None,1662)))\n",
    "    model.add(tf.keras.layers.LSTM(128, return_sequences=True, activation='relu'))\n",
    "    model.add(tf.keras.layers.LSTM(64, return_sequences=False, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(labels_array.shape[1], activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "701f987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efc01e7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "260/260 [==============================] - 131s 486ms/step - loss: nan - categorical_accuracy: 4.8117e-04\n",
      "Epoch 2/10\n",
      "260/260 [==============================] - 129s 495ms/step - loss: nan - categorical_accuracy: 6.0147e-04\n",
      "Epoch 3/10\n",
      "260/260 [==============================] - 128s 494ms/step - loss: nan - categorical_accuracy: 6.0147e-04\n",
      "Epoch 4/10\n",
      "260/260 [==============================] - 128s 493ms/step - loss: nan - categorical_accuracy: 6.0147e-04\n",
      "Epoch 5/10\n",
      "260/260 [==============================] - 129s 494ms/step - loss: nan - categorical_accuracy: 6.0147e-04\n",
      "Epoch 6/10\n",
      "260/260 [==============================] - 126s 486ms/step - loss: nan - categorical_accuracy: 6.0147e-04\n",
      "Epoch 7/10\n",
      "260/260 [==============================] - 129s 495ms/step - loss: nan - categorical_accuracy: 6.0147e-04\n",
      "Epoch 8/10\n",
      "260/260 [==============================] - 129s 497ms/step - loss: nan - categorical_accuracy: 6.0147e-04\n",
      "Epoch 9/10\n",
      "260/260 [==============================] - 127s 488ms/step - loss: nan - categorical_accuracy: 6.0147e-04\n",
      "Epoch 10/10\n",
      "260/260 [==============================] - 128s 494ms/step - loss: nan - categorical_accuracy: 6.0147e-04\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 64)          442112    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 128)         98816     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2000)              66000     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 662576 (2.53 MB)\n",
      "Trainable params: 662576 (2.53 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_lstm_model()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.fit(frames_array, labels_array, epochs= 10, callbacks=[tb_callback])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b91b15a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_word_folders(testing_path)  #folder for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06fdad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_videos():\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        print(video_metadata)\n",
    "        # for each word we find videos \n",
    "        for word in video_metadata:\n",
    "            if word['gloss'] in [\"book\",\"minute\",\"cherry\"]:\n",
    "                continue\n",
    "            for video in word['instances']:\n",
    "                if video['video_id'] in missing_videos or video['split'] != 'test': \n",
    "                    continue\n",
    "                video_path = os.path.join(testing_path, word['gloss'], video['video_id'])\n",
    "                os.makedirs(video_path, exist_ok=True)\n",
    "                cap = cv2.VideoCapture(archive_path + \"/\" + video['video_id'] + \".mp4\")\n",
    "                print(\"USO U FUNCKIJU FAJL :\" + archive_path + \"/\" + video['video_id'] + \".mp4\")\n",
    "                if video['frame_end'] == -1:\n",
    "                    frame_num = 1\n",
    "                    while True: \n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            break\n",
    "                        keypoints = extract_keypoints(frame, holistic)\n",
    "                        npy_path = os.path.join(video_path, str(frame_num))\n",
    "                        np.save(npy_path, keypoints)\n",
    "                        frame_num += 1\n",
    "                else:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, video['frame_start'])\n",
    "                    frame_num = video['frame_start']\n",
    "                    while frame_num <= video['frame_end']:\n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            break \n",
    "                        keypoints = extract_keypoints(frame, holistic)\n",
    "                        npy_path = os.path.join(video_path, str(frame_num))\n",
    "                        np.save(npy_path, keypoints)\n",
    "                        frame_num += 1 \n",
    "                        if frame_num > video['frame_end']:\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "165d08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_test_videos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e8b316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TESTING\n",
    "def load_data_from_folders_test(): \n",
    "    frames = []\n",
    "    labels = []\n",
    "    word_folders = sorted([f for f in os.listdir(testing_path) if os.path.isdir(os.path.join(testing_path, f)) and f != '.DS_Store'])\n",
    "    #testing_list = [\"book\",\"minute\",\"cherry\"]\n",
    "    for word_folder in word_folders:\n",
    "        word_path = os.path.join(testing_path, word_folder)\n",
    "        video_folders = sorted([f for f in os.listdir(word_path) if os.path.isdir(os.path.join(word_path, f)) and f != '.DS_Store'])\n",
    "        for video_folder in video_folders:\n",
    "            video_path = os.path.join(word_path, video_folder)\n",
    "            frame_files = sorted(os.listdir(video_path))\n",
    "            video_frames = []\n",
    "            for frame_file in frame_files:\n",
    "                frame_path = os.path.join(video_path, frame_file)\n",
    "                frame = np.load(frame_path)\n",
    "                video_frames.append(frame)\n",
    "            frames.append(video_frames)\n",
    "            labels.append(label_map[word_folder])\n",
    "    return frames, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba339922",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_test, labels_test = load_data_from_folders_test()\n",
    "print(labels)\n",
    "frames_test = tf.keras.preprocessing.sequence.pad_sequences(frames_test, padding='post', dtype='float32')\n",
    "labels_test = tf.keras.utils.to_categorical(labels_test)\n",
    "# Convert the padded_frames to a NumPy array\n",
    "frames_array_test = np.array(frames_test)\n",
    "labels_array_test = np.array(labels_test)\n",
    "print(frames_array_test.shape)\n",
    "print(labels_array_test.shape)\n",
    "#print(labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa841bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_predicted = model.predict(frames_array_test)\n",
    "multilabel_confusion_matrix(np.argmax(labels_array_test, axis=1), np.argmax(words_predicted , axis=1))\n",
    "accuracy_score(np.argmax(labels_array_test, axis=1), np.argmax(words_predicted , axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history = model.history.history\n",
    "accuracy = history['categorical_accuracy']\n",
    "val_accuracy = history['val_categorical_accuracy']\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "plt.plot(epochs, accuracy, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9cc7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
