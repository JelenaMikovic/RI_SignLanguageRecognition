{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33510535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c104da",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/Users/danilobabic/Documents/GitHub/ri_train_data'\n",
    "words = []\n",
    "missing_videos = []\n",
    "video_metadata = []\n",
    "words_file = '/Users/danilobabic/Documents/GitHub/archive/wlasl_class_list.txt'\n",
    "missing_videos_file = '/Users/danilobabic/Documents/GitHub/archive/missing.txt'\n",
    "video_metadata_file = '/Users/danilobabic/Documents/GitHub/archive/WLASL_v0.3.json'\n",
    "archive_path = '/Users/danilobabic/Documents/GitHub/archive/videos'\n",
    "testing_path = '/Users/danilobabic/Documents/GitHub/testing'\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ace583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip().split('\\t')\n",
    "            words.append(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb8cecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_words(words_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae0909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_files(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            missing_videos.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d8969",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_missing_files(missing_videos_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b6ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_metadata(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea98ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadata = get_video_metadata(video_metadata_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7178f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_folders(path):\n",
    "    for word in words:\n",
    "        folder_path = os.path.join(path, word)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1814f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(frame, model):         \n",
    "    results = model.process(frame)                 \n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_videos():\n",
    "    #print(\"USO U FUNKCIJUUUUU\")\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        print(video_metadata)\n",
    "        # for each word we find videos \n",
    "        for word in video_metadata:\n",
    "            for video in word['instances']:\n",
    "                if video['video_id'] in missing_videos or video['split'] != 'train': \n",
    "                    continue\n",
    "                video_path = os.path.join(folder_path, word['gloss'], video['video_id'])\n",
    "                os.makedirs(video_path, exist_ok=True)\n",
    "                cap = cv2.VideoCapture(archive_path + \"/\" + video['video_id'] + \".mp4\")\n",
    "                print(\"USO U FUNCKIJU FAJL :\" + archive_path + \"/\" + video['video_id'] + \".mp4\")\n",
    "                if video['frame_end'] == -1:\n",
    "                    frame_num = 1\n",
    "                    while True: \n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            break\n",
    "                        keypoints = extract_keypoints(frame, holistic)\n",
    "                        npy_path = os.path.join(video_path, str(frame_num))\n",
    "                        np.save(npy_path, keypoints)\n",
    "                        frame_num += 1\n",
    "                else:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, video['frame_start'])\n",
    "                    frame_num = video['frame_start']\n",
    "                    while frame_num <= video['frame_end']:\n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            break \n",
    "                        keypoints = extract_keypoints(frame, holistic)\n",
    "                        npy_path = os.path.join(video_path, str(frame_num))\n",
    "                        np.save(npy_path, keypoints)\n",
    "                        frame_num += 1 \n",
    "                        if frame_num > video['frame_end']:\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da308827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_folders():\n",
    "    frames = []\n",
    "    labels = []\n",
    "    word_folders = sorted(os.listdir(folder_path))\n",
    "    #testing_list = [\"book\",\"minute\",\"cherry\"]\n",
    "    for word_folder in word_folders:\n",
    "        word_path = os.path.join(folder_path, word_folder)\n",
    "        video_folders = sorted([f for f in os.listdir(word_path) if os.path.isdir(os.path.join(word_path, f))])\n",
    "        for video_folder in video_folders:\n",
    "            video_path = os.path.join(word_path, video_folder)\n",
    "            frame_files = sorted(os.listdir(video_path))\n",
    "            video_frames = []\n",
    "            for frame_file in frame_files:\n",
    "                frame_path = os.path.join(video_path, frame_file)\n",
    "                frame = np.load(frame_path)\n",
    "                video_frames.append(frame)\n",
    "            frames.append(video_frames)\n",
    "            labels.append(label_map[word_folder])\n",
    "    return frames, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be996211",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c90a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, labels = load_data_from_folders()\n",
    "#print(frames)\n",
    "max_frame_length = max(len(frame) for frame in frames)\n",
    "print(labels)\n",
    "# Pad the frames to have a common length\n",
    "#padded_frames = tf.keras.preprocessing.sequence.pad_sequences(frames, maxlen=max_frame_length, padding='post', value=0.0)\n",
    "frames = tf.keras.preprocessing.sequence.pad_sequences(frames, padding='post', dtype='float32')\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "# Convert the padded_frames to a NumPy array\n",
    "frames_array = np.array(frames)\n",
    "labels_array = np.array(labels)\n",
    "print(frames_array.shape)\n",
    "print(labels_array.shape)\n",
    "#print(labels_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(64, return_sequences=True, activation='relu', input_shape=(None,1662)))\n",
    "    model.add(tf.keras.layers.LSTM(128, return_sequences=True, activation='relu'))\n",
    "    model.add(tf.keras.layers.LSTM(64, return_sequences=False, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(labels_array.shape[1], activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d9d47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = create_lstm_model()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.fit(frames_array, labels_array, epochs= 200, callbacks=[tb_callback])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_word_folders(testing_path)  #folder for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e573ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_videos():\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        print(video_metadata)\n",
    "        # for each word we find videos \n",
    "        for word in video_metadata:\n",
    "            if word['gloss'] in [\"book\",\"minute\",\"cherry\"]:\n",
    "                continue\n",
    "            for video in word['instances']:\n",
    "                if video['video_id'] in missing_videos or video['split'] != 'test': \n",
    "                    continue\n",
    "                video_path = os.path.join(testing_path, word['gloss'], video['video_id'])\n",
    "                os.makedirs(video_path, exist_ok=True)\n",
    "                cap = cv2.VideoCapture(archive_path + \"/\" + video['video_id'] + \".mp4\")\n",
    "                print(\"USO U FUNCKIJU FAJL :\" + archive_path + \"/\" + video['video_id'] + \".mp4\")\n",
    "                if video['frame_end'] == -1:\n",
    "                    frame_num = 1\n",
    "                    while True: \n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            break\n",
    "                        keypoints = extract_keypoints(frame, holistic)\n",
    "                        npy_path = os.path.join(video_path, str(frame_num))\n",
    "                        np.save(npy_path, keypoints)\n",
    "                        frame_num += 1\n",
    "                else:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, video['frame_start'])\n",
    "                    frame_num = video['frame_start']\n",
    "                    while frame_num <= video['frame_end']:\n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            break \n",
    "                        keypoints = extract_keypoints(frame, holistic)\n",
    "                        npy_path = os.path.join(video_path, str(frame_num))\n",
    "                        np.save(npy_path, keypoints)\n",
    "                        frame_num += 1 \n",
    "                        if frame_num > video['frame_end']:\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ee296",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_test_videos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3186ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TESTING\n",
    "def load_data_from_folders_test(): \n",
    "    frames = []\n",
    "    labels = []\n",
    "    word_folders = sorted(os.listdir(testing_path))\n",
    "    #testing_list = [\"book\",\"minute\",\"cherry\"]\n",
    "    for word_folder in word_folders:\n",
    "        word_path = os.path.join(testing_path, word_folder)\n",
    "        video_folders = sorted([f for f in os.listdir(word_path) if os.path.isdir(os.path.join(word_path, f))])\n",
    "        for video_folder in video_folders:\n",
    "            video_path = os.path.join(word_path, video_folder)\n",
    "            frame_files = sorted(os.listdir(video_path))\n",
    "            video_frames = []\n",
    "            for frame_file in frame_files:\n",
    "                frame_path = os.path.join(video_path, frame_file)\n",
    "                frame = np.load(frame_path)\n",
    "                video_frames.append(frame)\n",
    "            frames.append(video_frames)\n",
    "            labels.append(label_map[word_folder])\n",
    "    return frames, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05aee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_test, labels_test = load_data_from_folders_test()\n",
    "print(labels)\n",
    "frames_test = tf.keras.preprocessing.sequence.pad_sequences(frames_test, padding='post', dtype='float32')\n",
    "labels_test = tf.keras.utils.to_categorical(labels_test)\n",
    "# Convert the padded_frames to a NumPy array\n",
    "frames_array_test = np.array(frames_test)\n",
    "labels_array_test = np.array(labels_test)\n",
    "print(frames_array_test.shape)\n",
    "print(labels_array_test.shape)\n",
    "#print(labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338930fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_predicted = model.predict(frames_array_test)\n",
    "multilabel_confusion_matrix(np.argmax(labels_array_test, axis=1), np.argmax(words_predicted , axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
